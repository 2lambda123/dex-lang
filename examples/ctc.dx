'## Connectionist Temporal Classification
By Alex Graves et alia.

'[Link to paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf).

' This dynamic programming algorithm computes the probability of a particular sequences of labels
(without pauses included) given another sequence of label probabilities
(with pauses included), marginalizing over all possible combination of
pause lengths.
It's used for training speech-to-text models on unaligned training data.

'Most implementations of CTC compute the marginal logprob
and its gradients combining a forward and a backward pass.
However, as mentioned in the original paper, we should only
need the last step of the forward pass to compute the marginal
probability, and Dex's autodiff should produce the backward
pass automatically.  That makes this code much shorter than
most implementations.


def interleave (blank:v) (labels: m=>v) : (m & (Fin 2))=>v =
  -- Turns "text" into "t e x t " by first pairing each letter with a blank,
  -- then flattening the pairs back into a single-index table.
  pairs = for i. [labels.i, blank]
  for (i, j). pairs.i.j

def prepend (first: v) (seq: m=>v) : ({head:Unit | tail:m }=>v) =
  -- Concatenates a single element to the beginning of a sequence.
  for idx. case idx of
    {| head = () |} -> first
    {| tail = i  |} -> seq.i

def prepend_and_interleave (blank:v) (seq: m=>v) :
  ({head:Unit | tail:(m & (Fin 2))}=>v) =
  -- Turns "text" into " t e x t".
  -- The output of this function has a slightly complicated output type, which
  -- has size 1 + 2 * (size m).
  interleaved = interleave blank seq
  prepend blank interleaved

def clipidx (n:Type) -> (i:Int) : n =
  -- Returns element at 0 if less than zero.
  -- Ideally we could have an alternative
  -- to Fin that just clips the index at its bounds.
  fromOrdinal n (select (i < 0) 0 i)

def ctc (time : Type) ?-> (vocab : Type) ?-> (position : Type) ?->
        (dict: Eq vocab) ?=> (dict2: Eq position) ?=> (dict3: Eq time) ?=> (blank: vocab)
        (logits: time=>vocab=>Float) (labels: position=>vocab) : Float =
  ilabels = prepend_and_interleave blank labels
  n = {head: Unit | tail: (position & Fin 2)}

  normalized_logits = for t. logsoftmax logits.t

  log_prob_seq_t0 = for pos:n. 0.01

  same_as_last = \ilabels s. False
  safe_idx = \prev s. select (s >= 0) prev.(clipidx n s) 0.0

  update = \t:time prev:(n=>Float).
    for s:n.
      same_as_last ilabels s
      labar = max prev.s (safe_idx prev ((ordinal s) - 1))
      labar + normalized_logits.t.(ilabels.s)

  log_prob_seq_final = fold log_prob_seq_t0 update
  log_prob_seq_final.(1@_)

'### Demo

def randIdxNoZero (n:Type) -> (k:Key) : n =
  unif = rand k
  fromOrdinal n $ (1 + (FToI (floor ( unif * IToF ((size n) - 1)))))

vocab = Fin 6
position = Fin 3
blank = 0@vocab

-- Create random logits
time = Fin 4
logits = for i:time j:vocab. (randn $ ixkey2 (newKey 0) i j)

-- Create random labels
labels = for i:position. randIdxNoZero vocab (newKey (ordinal i))
:p labels
> [1@(Fin 6), 5@(Fin 6), 5@(Fin 6)]

-- Evaluate marginal probability of labels given logits
:p exp $ ctc blank logits labels
> 1.0398494e-3



'### Test

-- Check that the sum of p(labels|logits) sums to 1.0 over all possible labels.
-- They don't yet sum to one, however I'm not 100% sure about the
-- semantics of the marginal likelihood being computed, and whether
-- e.g. the summed-over labels should include blanks.


sums = for i:vocab. ctc blank logits [i]
sums

sum sums

--%passes
sum $ for i:vocab. ctc blank logits [i]

:p
  tmp = for i:vocab. ctc blank logits [i]
  withState 0.0 \s.
    for i:vocab.
      s := get s + tmp.i
